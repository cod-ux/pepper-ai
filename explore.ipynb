{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import toml\n",
    "import asyncio\n",
    "import operator\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "from io import StringIO\n",
    "import sys\n",
    "import streamlit as st\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "import phoenix as px\n",
    "from phoenix.trace.langchain import LangChainInstrumentor\n",
    "from typing import TypedDict, List, Tuple, Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Existing running Phoenix instance detected! Shutting it down and starting a new instance...\n",
      "Attempting to instrument while already instrumented\n"
     ]
    }
   ],
   "source": [
    "secrets = \"C:/Users/Administrator/Documents/github/reporter/secrets.toml\"\n",
    "github_secrets = \"secrets.toml\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = toml.load(secrets)[\"OPENAI_API_KEY\"]\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "# Phoenix init.\n",
    "\n",
    "session = px.launch_app()\n",
    "LangChainInstrumentor().instrument()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GraphState(TypedDict):\n",
    "    request: str # Set\n",
    "    source_path: str # Set\n",
    "    past_execs: List\n",
    "\n",
    "    tasks: str\n",
    "    past_tasks: List[str]\n",
    "    current_task: str\n",
    "    \n",
    "    error: str\n",
    "    messages: List\n",
    "    generation: str\n",
    "    iterations: int # Set\n",
    "    max_iterations: int # Set\n",
    "    reflect: str\n",
    "\n",
    "\n",
    "    response: str\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    tasks: str = Field(\n",
    "        description=\"Tasks to complete to fulfill the user request in string format\"\n",
    "        )\n",
    "\n",
    "class code(BaseModel):\n",
    "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "    imports: str = Field(description=\"Code block import statements\")\n",
    "    code: str = Field(description=\"Code block not including import statements\")\n",
    "    description: str = \"Schema for code solutions to execute openpyxl related user requests\"\n",
    "\n",
    "class answer(BaseModel):\n",
    "    ans: str = Field(\"Answer to the provided question\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Templates section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import toml\n",
    "import json\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "from openai import OpenAI\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from utils import (\n",
    "    query_llm_gpt4,\n",
    "    extract_code_from_llm,\n",
    "    load_excel_to_df,\n",
    "    load_sheets_to_dfs\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "secrets = \"C:/Users/Administrator/Documents/github/reporter/secrets.toml\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = toml.load(secrets)[\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "def planner_template():\n",
    "\n",
    "    system_msg = f\"\"\"\n",
    "You are an intelligent data analyst specializing in answering Excel file exploration related questions by making a plan with clear, actionable tasks that can be executed by coding in python's Pandas and seaborn modules (if graph's are involved) to answer the user question about the excel file.\n",
    "Analyze the user's request and create a concise list of steps a programmer should follow to fulfill the exploration question:\n",
    "\n",
    "Follow these guidelines:\n",
    "- Focus on essential tasks that directly achieve the user's goal. Avoid trivial steps like \"Open the Excel file.\" Ensure tasks are descriptive enough to be executed without further clarification and keeps the list brief.\n",
    "- The last step should ALWAYS be returning your findings by either printing them, if its text, or displaying chart if the output is a graph.\n",
    "- If the user questions is instead a request to change any source data from the excel file, Abandon all operations and ONLY print that you can only answer questions in the \"explore\" mode and can't manipulate the data. \\n\n",
    "- The last section of the program should always be a print statement that provides a meaningful word response to the user question.\n",
    "- Try not to print the dataframe or its head in the answers\n",
    "\"\"\"\n",
    "\n",
    "    template = [\n",
    "        (\n",
    "            'system',\n",
    "            system_msg\n",
    "        ),\n",
    "        (\n",
    "            \"placeholder\",\n",
    "            \"{messages}\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(template)\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def format_request(request, source):\n",
    "    dfs, sheet_names = load_sheets_to_dfs(source)\n",
    "    head_view = ''\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        head_view += f\"\\nSheet {i}: {sheet_names[i]}\\nSheet head:\\n{df.head(3)}\\n\\n\"\n",
    "\n",
    "    formatted = f\"\"\"\n",
    "The user wants to find an answer to their question regarding this excel file called: {source}.\\n\n",
    "\n",
    "------------\n",
    "There are {len(dfs)} sheets in the excel file. Here is how the first few rows of those sheets look like:\n",
    "{head_view}\n",
    "------------\n",
    "\n",
    "User request: {request}\"\"\"\n",
    "\n",
    "    return formatted\n",
    "\n",
    "\n",
    "def retrieve_context(request, retriever):\n",
    "    code_examples = retriever.invoke(f\"Documentation related to : {request}\")\n",
    "    content = [doc.page_content for doc in code_examples]\n",
    "    seperator = \"\\n\\n\\n-----\\n\\n\\n\"\n",
    "    conc_content = seperator.join(content)\n",
    "\n",
    "    return conc_content\n",
    "\n",
    "def code_chain_template():\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                'system',\n",
    "                \"\"\"You are a coding assistant with expertise in Python's pandas module and seaborn module.\\n\n",
    "Answer the user question about the excel file by writing code for executing each task from the generated plan. Ensure that any code you provie can be executed \\n\n",
    "with all required imports and variables defined. Structure your response with a description of the code solution. \\n\n",
    "Then list the imports. And finally list the functioning code block. NEVER make any changes to the source excel file. \\n\n",
    "\n",
    "If the user questions is instead a request to change any source data from the excel file, ONLY write the python code to print that you can only answer questions in \"explore\" mode and can't manipulate the data. \\n\n",
    "\n",
    "\n",
    "Remember that you are in a streamlit envirnonment, so follow the guidelines when returning the results:\n",
    "- ALWAYS do return the python code to import pandas, even if code is not required. \\n\n",
    "- ALWAYS dp print the answer that you find at the end of the program. This will provide the user with the answer to their question. \\n\n",
    "- ALWAYS do save any graphs you make with seaborn as images in the folder \"C:/Users/Administrator/Documents/github/reporter/ph_images\" at the end of the program. This will provide the user with the answer to their question. \\n\n",
    "- ALWAYS do write code to create some sort of a graph whenever the user asks for wrtiting a graph. \\n\n",
    "\n",
    "Here is the user's original question, progress on executing previous tasks, and the current task you need to write code to execute. Write code to execute the last retrieved task from the plan: \"\"\"\n",
    "            ),\n",
    "            (\n",
    "                'placeholder',\n",
    "                '{messages}'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def data_analyst_template():\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                'system',\n",
    "                \"\"\"You are a data analyst who is an expert in understanding data and coding, and provides logical answers to user questions.\\n\n",
    "Answer the user question about the excel file by referring to the conversation between the user and the system, \\n\n",
    "providing a logical and concise answer to the user's question. \\n\n",
    "Do not explain the code or return any code in the final answer. \\n\n",
    "Do provide a status update on whether or not the system was able to execute the request and if any errors, explain why the system got into an error. \\n\n",
    "\n",
    "Never include any file path names or information about saving images in the final response: \"\"\"\n",
    "            ),\n",
    "            (\n",
    "                'placeholder',\n",
    "                '{messages}'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing code_chain()\n",
    "\n",
    "code_prompt = code_chain_template()\n",
    "code_chain = code_prompt | llm.with_structured_output(code)\n",
    "analyst_prompt = data_analyst_template()\n",
    "analyst_chain = analyst_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plan_steps(state: GraphState):\n",
    "    messages = state[\"messages\"]\n",
    "    past_tasks = state[\"past_tasks\"]\n",
    "    past_execs = state[\"past_execs\"]\n",
    "    reflect = state[\"reflect\"]\n",
    "    error = state[\"error\"]\n",
    "    iteration = state[\"iterations\"]\n",
    "    past_tasks = []\n",
    "    reflect = ''\n",
    "\n",
    "    if error == \"yes\":\n",
    "        messages += [(\"system\", f\"Now I should try again to recreate a plan that doesn't without producing any errors, if the error was due to the plan. Let's rewrite a new plan if necessary.\")]\n",
    "\n",
    "    if iteration == 0:\n",
    "        if len(past_execs) > 4:\n",
    "            exec_string = ''\n",
    "            for execs in past_execs[-4:0]:\n",
    "                req = execs[\"question\"]\n",
    "                table = execs[\"answer\"]\n",
    "                exec_string += f\"\\nPast Question: {req}\\Answer: {table}\\n\"\n",
    "            messages += [(\"user\", f\"These are the past 4 questions from me, the user, along with the answers you provided: \\n{exec_string}\")]\n",
    "\n",
    "        if len(past_execs) < 4:\n",
    "            exec_string = ''\n",
    "            for execs in past_execs:\n",
    "                req = execs[\"question\"]\n",
    "                table = execs[\"answer\"]\n",
    "                exec_string += f\"\\Past Question: {req}\\Answer: {table}\\n\"\n",
    "            messages += [(\"system\", f\"These are the past few questions from me, the user, along with the answers you provided: \\n{exec_string}\")]\n",
    "\n",
    "\n",
    "    planner_prompt = planner_template()\n",
    "    planner = create_structured_output_runnable(Plan, ChatOpenAI(model=\"gpt-4o\", temperature=0, streaming=True), planner_prompt)\n",
    "\n",
    "    print(\"\\nPlanning step begins...\\n\")\n",
    "\n",
    "    plan = planner.invoke({\"messages\": messages})\n",
    "\n",
    "    messages += [('system', f\"For the user questions, this is the list of tasks I need to complete to find the answer: \\n{plan.tasks}\")]\n",
    "    \n",
    "    return {\"tasks\": plan.tasks, \"messages\": messages, \"past_tasks\": past_tasks, \"reflect\": reflect, \"current_task\": f\"Plan completed...\"}\n",
    "\n",
    "# Function: Write code\n",
    "\n",
    "def generate_code(state: GraphState):\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    error = state[\"error\"]\n",
    "    generation = state[\"generation\"]\n",
    "    tasks = state[\"tasks\"]\n",
    "    past_tasks = state[\"past_tasks\"]\n",
    "\n",
    "    print(\"Generating code....\")\n",
    "\n",
    "    if error == \"yes\":\n",
    "        messages += [(\"system\", f\"Now I should try again to generate code with the newly written plan that wouldn't produce any errors during execution, in case the error was due to the written code. If the error produced was not because of the code, I will use the same code. This is the plan to follow: \\n{tasks}\")]\n",
    "\n",
    "    messages += [(\"system\", \"I need to follow the plan to write a readily executable program, which I will execute later, to print the answer to the user's question.\")]\n",
    "    generation = code_chain.invoke({\"messages\": messages})\n",
    "\n",
    "    messages += [('system', f\"This is the code solution written based ont the plan to answer the user question:\\n{generation.imports}\\n{generation.code}\")]\n",
    "\n",
    "    iterations += 1\n",
    "    print(\"Reached end of generation...\")\n",
    "\n",
    "    #last_msgs = messages[-2:]\n",
    "    #last_indx = -2*len(task)\n",
    "    #messages = messages[:last_indx] + last_msgs\n",
    "    return {\"generation\": generation, \"messages\": messages, \"iterations\": iterations, \"current_task\": \"Executing code...\"}\n",
    "\n",
    "def check_code(state: GraphState):\n",
    "    print(\"Code is being executed....\\n--------\\n\")\n",
    "    messages = state[\"messages\"]\n",
    "    generation = state[\"generation\"]\n",
    "    error = state[\"error\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    reflect = state[\"reflect\"]\n",
    "    response = [\"response\"]\n",
    "\n",
    "    try:\n",
    "        exec(generation.imports)\n",
    "\n",
    "    except Exception as e:\n",
    "        error = \"yes\"\n",
    "        messages += [(\"system\", f\"Encountered an error with import statement: {e}\")]\n",
    "\n",
    "        print(f\"Error with import statement: {e}\")\n",
    "        return {\"error\": error, \"messages\": messages, \"current_task\": \"An error has occured. Retrying Solution...\"}\n",
    "\n",
    "    captured_output = StringIO()\n",
    "    sys.stdout = captured_output\n",
    "\n",
    "    try:\n",
    "        print(generation.code)\n",
    "        exec(generation.code)\n",
    "\n",
    "    except Exception as e:\n",
    "        error = \"yes\"\n",
    "        messages += [(\"system\", f\"Encountered an error with code block statement: {e}\")]\n",
    "\n",
    "        print(f\"Error with code block: {e}\")\n",
    "        return {\"error\": error, \"messages\": messages, \"current_task\": \"An error has occured. Retrying Solution...\"}\n",
    "\n",
    "    captured_output = captured_output.getvalue()\n",
    "    sys.stdout = sys.__stdout__\n",
    "\n",
    "    response = captured_output\n",
    "    messages += [(\"system\", f\"Printed from execution: {response}\")]\n",
    "\n",
    "    error = \"no\"\n",
    "    print(\"\\n--------\\n\")\n",
    "\n",
    "    if iterations > 2:\n",
    "        reflect = \"yes\"\n",
    "\n",
    "    print(\"-----NO CODE TEST FAILURES-----\")\n",
    "    return {\"messages\": messages, \"error\": error, \"reflect\": reflect, \"response\": response, \"current_task\": \"Typing Answer...\"}\n",
    "\n",
    "def reflect_code(state: GraphState):\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    messages += [\n",
    "        (\n",
    "            'system',\n",
    "            \"\"\"\n",
    "            I tried to solve the problem and failed a unit test. I need to reflect on this failure based on the generated error.\\n\n",
    "            Write a few key suggestions to avoid making this mistake again.\n",
    "            \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    reflections = code_chain.ainvoke({\"messages\": messages})\n",
    "    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n",
    "\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def write_answer(state: GraphState):\n",
    "    messages = state[\"messages\"]\n",
    "    response = state[\"response\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    result = analyst_chain.invoke({\"messages\": messages})\n",
    "\n",
    "    response = result.content\n",
    "    messages += [(\"system\", f\"Here is the answer to the user's question: \\n{response}\")]\n",
    "\n",
    "    generation_string = f\"{generation.imports}\\n{generation.code}\"\n",
    "\n",
    "    print(\"Response:\\n\", response, \"\\n\")\n",
    "\n",
    "    return {\"response\": response, \"messages\": messages, \"current_task\": \"Closing...\", \"generation\": f\"{generation_string}\"}\n",
    "\n",
    "\n",
    "def should_end(state: GraphState):\n",
    "    error = state[\"error\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    max_iters = state[\"max_iterations\"]\n",
    "    reflect = state[\"reflect\"]\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    if error == \"no\" or iterations == max_iters:\n",
    "        print(\"----DECISION: FINISH----\")\n",
    "        #print(\"Messages: \\n\", messages)\n",
    "        return \"end\"\n",
    "        \n",
    "\n",
    "    else:\n",
    "        print(\"----DECISION: RE-TRY SOLUTION----\")\n",
    "        if reflect == \"yes\":\n",
    "            print(\"Reflecting on error...\")\n",
    "            return \"reflect_code\"\n",
    "\n",
    "        else:\n",
    "            return \"planner\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wf = StateGraph(GraphState)\n",
    "\n",
    "wf.add_node(\"planner\", plan_steps)\n",
    "wf.add_node(\"generate\", generate_code)\n",
    "wf.add_node(\"check_code\", check_code)\n",
    "wf.add_node(\"reflect_code\", reflect_code)\n",
    "wf.add_node(\"write_answer\", write_answer)\n",
    "\n",
    "wf.set_entry_point(\"planner\")\n",
    "wf.add_edge(\"planner\", \"generate\")\n",
    "wf.add_edge(\"generate\", \"check_code\")\n",
    "wf.add_conditional_edges(\n",
    "    \"check_code\",\n",
    "    should_end,\n",
    "    {\n",
    "        \"end\": \"write_answer\",\n",
    "        \"reflect_code\": \"reflect_code\",\n",
    "        \"planner\": \"planner\",\n",
    "    }\n",
    ")\n",
    "wf.add_edge(\"reflect_code\", \"planner\")\n",
    "wf.add_edge(\"write_answer\", END)\n",
    "\n",
    "explore = wf.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'C:/Users/Administrator/Documents/github/reporter/sales_data_copy.xlsx'\n",
    "\n",
    "def main():\n",
    "    df = pd.read_excel(source)\n",
    "    print(df.head(7))\n",
    "    executions = []\n",
    "    while True:\n",
    "        req = input(\">> \")\n",
    "        formatted_req = format_request(req, source)\n",
    "        temp = []\n",
    "\n",
    "        inputs = {\"messages\": [('user', formatted_req)], \"request\": req, \"source_path\": source, \"max_iterations\": 3, \"iterations\": 0, \"past_execs\": executions, \"response\": \"\"}\n",
    "        msg = explore.invoke(inputs)\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
